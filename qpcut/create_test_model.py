import numpy as np
import os
import torch
import sys
import pickle

# manually add VeriNet root directory to the $PYTHONPATH
SCRIPT_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
# print(SCRIPT_DIR)
sys.path.append(SCRIPT_DIR)
# print(sys.path)

from verinet.neural_networks.verinet_nn import VeriNetNN, VeriNetNNNode
from sdpbab.calculate_bounds import BCalculator

epsilon = 0.1

layer_types = ["Linear", "ReLU", "Linear"]

weights = []
weights.append(np.array([
    [1.0, -2.0],
    [-3.0, 4.0]
], dtype=np.float32))
weights.append(np.array([
    [4.0, -3.0],
    [-2.0, 1.0]
], dtype=np.float32))
biases = []
biases.append(np.array([1.0, -1.0], dtype=np.float32))
biases.append(np.array([1.0, -1.0], dtype=np.float32))
print(weights)
print(biases)

# Create VeriNetNN model
nodes = []
index = 0
weight_index = 0
nodes.append(VeriNetNNNode(index, torch.nn.Identity(), [], [index+1]))
index += 1
for layer_type in layer_types:
    if layer_type == "Linear":
        new_op = torch.nn.Linear(weights[weight_index].shape[1], weights[weight_index].shape[0])
        with torch.no_grad():
            new_op.weight = torch.nn.Parameter(torch.tensor(weights[weight_index]))
            new_op.bias = torch.nn.Parameter(torch.tensor(biases[weight_index]))
        nodes.append(VeriNetNNNode(index, new_op, [index-1], [index+1]))
        index += 1
        weight_index += 1
    elif layer_type == "ReLU":
        nodes.append(VeriNetNNNode(index, torch.nn.ReLU(), [index - 1], [index + 1]))
        index += 1


nodes.append(VeriNetNNNode(index, torch.nn.Identity(), [index-1], []))

model = VeriNetNN(nodes)
print(model)

# Save model to onnx
# model.save requires a dummy tensor with the same shape as the inputs to the network --> generated by torch.randn()

model.save(torch.randn(1, weights[0].shape[1]), "test.onnx")

weights = []
biases = []
layer_types = []

# Read model from file and save its structure, weights and biases
for (idx, node) in enumerate(model.nodes):
    if isinstance(node.op, torch.nn.modules.flatten.Flatten):
        layer_types.append("Flatten")
    elif isinstance(node.op, torch.nn.modules.linear.Linear):
        layer_types.append("Linear")
        weights.append(node.op.weight.detach().numpy())
        biases.append(node.op.bias.detach().numpy())
    elif isinstance(node.op, torch.nn.modules.activation.ReLU):
        layer_types.append("ReLU")
    elif isinstance(node.op, torch.nn.Identity):
        # do nothing (Identity layers are automatically appended at the input/output of the NN below)
        pass
    else:
        raise NotImplementedError("Processing for layer " + str(idx) + " of type " + str(node.op) +
                                  " not implemented")

# Load Dataset
input_image = torch.ones([2, 1]) + 10
label = torch.tensor([0])
n_classes = 2
image = 0
# build BCalculator for shrunk_model
calculator = BCalculator(model, [1, 2, 1])  # batch_size, dim_1, dim_2

for j in range(n_classes):
    export_path = os.path.join("data", "export", "test" + "_{}".format(epsilon), "image_{}".format(image))
    if not os.path.exists(export_path):  # create export directory
        os.makedirs(export_path)

    lower_bounds = torch.flatten(input_image) - epsilon
    upper_bounds = torch.flatten(input_image) + epsilon
    inp_bounds = torch.stack((lower_bounds, upper_bounds), dim=-1)

    counter = 0
    lb_pre = []
    ub_pre = []
    weight_list = []
    bias_list = []
    for idx, node in enumerate(model.nodes):
        if isinstance(node.op, torch.nn.modules.linear.Linear) or isinstance(node.op, torch.nn.Conv2d):
            # idx+1 because this calculates the pre-layer bounds while we want the bounds after the linear layer
            bounds = calculator.calculate_bounds(inp_bounds, layer=idx + 1).numpy()
            weights = node.op.weight.detach().numpy()
            biases = node.op.bias.detach().numpy()

            # save data in a dict
            counter = counter + 1
            lb_pre.append(bounds[:, 0])
            ub_pre.append(bounds[:, 1])
            weight_list.append(weights)
            bias_list.append(biases)
    # save everything as separate files
    print("Saving Files for Image {}".format(image))
    # n_layers
    with open(os.path.join(export_path, "n_layers.pkl"), "wb") as f:
        pickle.dump(counter, f)

    # bias
    bias = []
    for i in range(counter):
        bias.append(bias_list[i].reshape(-1, 1).astype(np.double))
    with open(os.path.join(export_path, "bias.pkl"), "wb") as f:
        pickle.dump(bias, f)

    # lpost  --> post activation bounds, so need to apply ReLU function
    lpost = [np.maximum(0, lower_bounds.numpy()).reshape(-1, 1).astype(
        np.double)]  # post-activation bounds of zero-th layer = input bounds
    for i in range(1, counter):
        lpost.append(np.maximum(0, lb_pre[i - 1]).reshape(-1, 1).astype(np.double))
        with open(os.path.join(export_path, "lpost.pkl"), "wb") as f:
            pickle.dump(lpost, f)

    # lpre
    lpre = []
    for i in range(counter - 1):
        lpre.append(lb_pre[i].reshape(-1, 1).astype(np.double))  # apply ReLU
    with open(os.path.join(export_path, "lpre.pkl"), "wb") as f:
        pickle.dump(lpre, f)

    # sizes
    sizes = [bias_list[i].shape[0] for i in range(len(bias_list))]
    sizes.insert(0, weight_list[0].shape[1])
    with open(os.path.join(export_path, "sizes.pkl"), "wb") as f:
        pickle.dump(np.array(sizes, dtype=np.int64), f)

    # upost
    upost = [np.maximum(0, upper_bounds.numpy()).reshape(-1, 1).astype(
        np.double)]  # post-activation bounds of zero-th layer = input bounds
    for i in range(1, counter):
        upost.append(np.maximum(0, ub_pre[i - 1]).reshape(-1, 1).astype(np.double))
    with open(os.path.join(export_path, "upost.pkl"), "wb") as f:
        pickle.dump(upost, f)

    # upre
    upre = []
    for i in range(counter - 1):
        upre.append(ub_pre[i].reshape(-1, 1).astype(np.double))  # apply ReLU
    with open(os.path.join(export_path, "upre.pkl"), "wb") as f:
        pickle.dump(upre, f)

    # Weights
    wts = []
    for i in range(counter):
        wts.append(weight_list[i].astype(np.double))
    with open(os.path.join(export_path, "wts.pkl"), "wb") as f:
        pickle.dump(wts, f)

    # True Label
    with open(os.path.join(export_path, "label.pkl"), "wb") as f:
        pickle.dump(label.numpy().item(), f)

    # Image itself
    with open(os.path.join(export_path, "input_image.pkl"), "wb") as f:
        pickle.dump(input_image.numpy(), f)
